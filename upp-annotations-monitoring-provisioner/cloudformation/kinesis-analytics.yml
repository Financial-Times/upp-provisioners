AWSTemplateFormatVersion: '2010-09-09'
Description: This template creates a kinesis analytics application, as a POC for Annotations Monitoring

Parameters:
  InputKinesisStreamARN:
      Description: ARN for the Content fluentd kinesis stream we will use as an input.
      Type: String

  InputRoleARN:
      Description: ARN for the IAM Role which grants access to the configured Kinesis stream
      Type: String

  AnalyticsOutputRoleARN:
      Description: ARN for the IAM Role which allows the analytics application to output to firehose.
      Type: String

  FirehoseS3RoleARN:
      Description: ARN for the IAM Role which allows the firehose stream to write to an S3 bucket.
      Type: String

  TagTeamDL:
      Description: Tag of the TeamDL
      Type: String
      AllowedPattern: ^([a-zA-Z0-9_\-\.]+)@([a-zA-Z0-9_\-\.]+)\.([a-zA-Z]{2,5})$
      ConstraintDescription: There must be a valid email address for the TeamDL
      Default: universal.publishing.platform@ft.com

  TagEnvironment:
      Description: Tag detail for the Environment
      Type: String
      AllowedValues:
          - 't'
          - 'p'
          - 'd'
      Default: t

  TagSystemCode:
      Description: The system code for the environment
      Type: String
      Default: annotations-monitoring

  TagIpCode:
      Description: The environment ipCode
      Type: String
      AllowedPattern: '[P][0-9]*'
      Default: P196

  TagDescription:
      Description: Tag detail for the describing the instance
      Type: String
      Default: Kinesis analytics application used to monitor Annotations publishes

  TagName:
      Description: Tag detail for the Name used in the console of the instance
      Type: String
      Default: Annotations Monitoring Kinesis Analytics Application

  EnvironmentName:
    Description: An environment name that will be prefixed to resource names
    Type: String

Resources:
  AnnotationsMonitoringAnalytics:
    Type: "AWS::KinesisAnalytics::Application"
    Properties:
      ApplicationName: !Sub ${EnvironmentName}-analytics-application
      ApplicationDescription: !Ref TagDescription
      Inputs:
        - NamePrefix: "content_fluentd_input"
          InputSchema:
            RecordColumns:
              - Name: "event_time"
                SqlType: "TIMESTAMP"
                Mapping: "$.event_time"
              - Name: "content_type"
                SqlType: "VARCHAR(64)"
                Mapping: "$.content_type"
              - Name: "event"
                SqlType: "VARCHAR(32)"
                Mapping: "$.event"
              - Name: "monitoring_event"
                SqlType: "BOOLEAN"
                Mapping: "$.monitoring_event"
              - Name: "service_name"
                SqlType: "VARCHAR(64)"
                Mapping: "$.service_name"
              - Name: "transaction_id"
                SqlType: "VARCHAR(128)"
                Mapping: "$.transaction_id"
              - Name: "uuid"
                SqlType: "VARCHAR(36)"
                Mapping: "$.uuid"
              - Name: "isValid"
                SqlType: "BOOLEAN"
                Mapping: "$.isValid"
            RecordFormat:
              RecordFormatType: "JSON"
              MappingParameters:
                JSONMappingParameters:
                  RecordRowPath: "$"
          KinesisStreamsInput:
            ResourceARN: !Ref InputKinesisStreamARN
            RoleARN: !Ref InputRoleARN
      ApplicationCode: |
        -- Annotations Monitoring Analytics Application:
        --    Creates three in-application streams, one for PublishStart events, one for Map events and one for SaveNeo4j events.
        --    Two output streams, one which will join the above three streams to produce a completed event output.
        --    One will join the three streams and monitor for failed events

        -- Create PublishStart event stream

        CREATE OR REPLACE STREAM "PUBLISH_START_STREAM" (
           "uuid" varchar(64),
           "transaction_id" varchar(64),
           "content_type" varchar(64),
           "event" varchar(16),
           "event_time" timestamp
        );

        CREATE OR REPLACE PUMP "PUBLISH_START_PUMP" AS INSERT INTO "PUBLISH_START_STREAM"

        SELECT STREAM "uuid", "transaction_id", "content_type", "event", "event_time"
        FROM "content_fluentd_input_001"
        WHERE ("event" = 'PublishStart' AND "content_type" = 'Annotations');


        -- Create SaveNeo4j event stream

        CREATE OR REPLACE STREAM "SAVE_NEO4J_STREAM" (
           "uuid" varchar(64),
           "transaction_id" varchar(64),
           "content_type" varchar(64),
           "event" varchar(16),
           "event_time" timestamp
        );

        CREATE OR REPLACE PUMP "SAVE_NEO4J_PUMP" AS INSERT INTO "SAVE_NEO4J_STREAM"

        SELECT STREAM "uuid", "transaction_id", "content_type", "event", "event_time"
        FROM "content_fluentd_input_001"
        WHERE ("event" = 'SaveNeo4j' AND "content_type" = 'Annotations');


        -- Create Map event stream

        CREATE OR REPLACE STREAM "MAP_STREAM" (
           "uuid" varchar(64),
           "transaction_id" varchar(64),
           "content_type" varchar(64),
           "event" varchar(16),
           "event_time" timestamp,
           "isValid" boolean
        );

        CREATE OR REPLACE PUMP "MAP_PUMP" AS INSERT INTO "MAP_STREAM"

        SELECT STREAM  "uuid", "transaction_id", "content_type", "event", "event_time", "isValid"
        FROM "content_fluentd_input_001"
        WHERE ("event" = 'Map' AND "content_type" = 'Annotations');


        -- Join the three streams to determine completed publishes

        CREATE OR REPLACE STREAM "SUCCESSFUL_PUBLISHES_STREAM" (
            "uuid" varchar(64),
            "transaction_id" varchar(64),
            "content_type" varchar(64),
            "publish_start_time" timestamp,
            "map_time" timestamp,
            "save_time" timestamp
        );

        -- The following query will watch the stream for any of the three events, and attempt to match them
        -- to other events within the SLA window. If all three events have not occurred during the window,
        -- the query will not return an output row for that publish.

        CREATE OR REPLACE PUMP "SUCCESSFUL_PUBLISHES_STREAM_PUMP" AS
        INSERT INTO "SUCCESSFUL_PUBLISHES_STREAM"
           SELECT STREAM
              COALESCE(p."uuid", m."uuid", s."uuid") as "uuid",
              COALESCE(p."transaction_id", m."transaction_id", s."transaction_id") as "transaction_id",
              COALESCE(p."content_type", m."content_type", s."content_type") as "content_type",
              p."event_time" as "publish_start_time",
              m."event_time" as "map_time",
              s."event_time" as "save_time"
           FROM PUBLISH_START_STREAM OVER SLA AS p
              JOIN SAVE_NEO4J_STREAM OVER SLA AS s ON (p."transaction_id" = s."transaction_id" AND p."uuid" = s."uuid")
              JOIN MAP_STREAM OVER SLA AS m ON (p."transaction_id" = m."transaction_id" AND p."uuid" = m."uuid")
           WINDOW SLA AS (RANGE INTERVAL '2' MINUTE PRECEDING);


        -- Create stream for publish analysis

        CREATE OR REPLACE STREAM "PUBLISH_ANALYSIS_STREAM" (
            "row_time" timestamp,
            "count" int,
            "average_duration_ms" double,
            "min_duration" int,
            "max_duration" int
        );

        -- The following query will aggregate and analyse successful publishes in each 5min interval

        CREATE OR REPLACE PUMP "PUBLISH_ANALYSIS_STREAM_PUMP" AS
        INSERT INTO "PUBLISH_ANALYSIS_STREAM"
           SELECT STREAM
              FIRST_VALUE(p.ROWTIME) as "row_time",
              COUNT(*),
              AVG(TSDIFF(s."event_time", p."event_time")),
              MIN(TSDIFF(s."event_time", p."event_time")),
              MAX(TSDIFF(s."event_time", p."event_time"))
           FROM PUBLISH_START_STREAM AS p
              JOIN SAVE_NEO4J_STREAM AS s ON (p."transaction_id" = s."transaction_id" AND p."uuid" = s."uuid")
              JOIN MAP_STREAM AS m ON (p."transaction_id" = m."transaction_id" AND p."uuid" = m."uuid")
           GROUP BY
              STEP(p.ROWTIME BY INTERVAL '5' MINUTE);

        -- Join the three streams to determine failed publishes

        CREATE OR REPLACE STREAM "FAILED_PUBLISHES_STREAM" (
            "uuid" varchar(64),
            "transaction_id" varchar(64),
            "content_type" varchar(64),
            "publish_start_time" timestamp,
            "map_time" timestamp,
            "save_time" timestamp
        );

        -- The following query will monitor for rows in any of the three streams, attempt to join
        -- events within them, and report if the join contains null timestamps for the Map or SaveNeo4j
        -- events. The join will wait for the duration of the SLA window before considering a row as failed.

        CREATE OR REPLACE PUMP "FAILED_PUBLISHES_STREAM_PUMP" AS
        INSERT INTO "FAILED_PUBLISHES_STREAM"
           SELECT STREAM
              COALESCE(p."uuid", m."uuid", s."uuid") as "uuid",
              COALESCE(p."transaction_id", m."transaction_id", s."transaction_id") as "transaction_id",
              COALESCE(p."content_type", m."content_type", s."content_type") as "content_type",
              p."event_time" as "publish_start_time",
              m."event_time" as "map_time",
              s."event_time" as "save_time"
           FROM PUBLISH_START_STREAM OVER SLA AS p
              LEFT OUTER JOIN SAVE_NEO4J_STREAM OVER SLA AS s ON (p."transaction_id" = s."transaction_id" AND p."uuid" = s."uuid")
              LEFT OUTER JOIN MAP_STREAM OVER SLA AS m ON (p."transaction_id" = m."transaction_id" AND p."uuid" = m."uuid")
           WHERE (m."isValid" AND s."event_time" is null AND m."event_time" is not null)
           WINDOW SLA AS (RANGE INTERVAL '2' MINUTE PRECEDING);

  AnnotationsMonitoringS3Bucket:
    Type: "AWS::S3::Bucket"
    Properties:
      BucketName: "annotations-monitoring-dev-poc-s3"

  AnnotationsMonitoringS3FirehoseFailures:
    Type: "AWS::KinesisFirehose::DeliveryStream"
    DependsOn: AnnotationsMonitoringS3Bucket
    Properties:
      DeliveryStreamName: !Sub ${EnvironmentName}-failures-poc-s3-firehose
      S3DestinationConfiguration:
        BucketARN: !GetAtt AnnotationsMonitoringS3Bucket.Arn
        BufferingHints:
          IntervalInSeconds: "900"
          SizeInMBs: "100"
        CompressionFormat: "UNCOMPRESSED"
        Prefix: failures/
        RoleARN: !Ref FirehoseS3RoleARN

  AnnotationsMonitoringS3FirehoseSuccesses:
    Type: "AWS::KinesisFirehose::DeliveryStream"
    DependsOn: AnnotationsMonitoringS3Bucket
    Properties:
      DeliveryStreamName: !Sub ${EnvironmentName}-successes-poc-s3-firehose
      S3DestinationConfiguration:
        BucketARN: !GetAtt AnnotationsMonitoringS3Bucket.Arn
        BufferingHints:
          IntervalInSeconds: "900"
          SizeInMBs: "100"
        CompressionFormat: "UNCOMPRESSED"
        Prefix: successes/
        RoleARN: !Ref FirehoseS3RoleARN

  # AnnotationsMonitoringS3FirehoseErrors:
    # Type: "AWS::KinesisFirehose::DeliveryStream"
    # DependsOn: AnnotationsMonitoringS3Bucket
    # Properties:
      # DeliveryStreamName: !Sub ${EnvironmentName}-errors-poc-s3-firehose
      # S3DestinationConfiguration:
        # BucketARN: !GetAtt AnnotationsMonitoringS3Bucket.Arn
        # BufferingHints:
          # IntervalInSeconds: "900"
          # SizeInMBs: "100"
        # CompressionFormat: "UNCOMPRESSED"
        # Prefix: errors/
        # RoleARN: !Ref FirehoseS3RoleARN

  AnnotationsMonitoringS3FirehosePublishAnalysis:
    Type: "AWS::KinesisFirehose::DeliveryStream"
    DependsOn: AnnotationsMonitoringS3Bucket
    Properties:
      DeliveryStreamName: !Sub ${EnvironmentName}-analysis-poc-s3-firehose
      S3DestinationConfiguration:
        BucketARN: !GetAtt AnnotationsMonitoringS3Bucket.Arn
        BufferingHints:
          IntervalInSeconds: "900"
          SizeInMBs: "100"
        CompressionFormat: "UNCOMPRESSED"
        Prefix: analysis/
        RoleARN: !Ref FirehoseS3RoleARN

  AnnotationsMonitoringAnalyticsOutputFailures:
    Type: "AWS::KinesisAnalytics::ApplicationOutput"
    DependsOn: AnnotationsMonitoringAnalytics
    Properties:
      ApplicationName: !Ref AnnotationsMonitoringAnalytics
      Output:
        Name: FAILED_PUBLISHES_STREAM
        DestinationSchema:
          RecordFormatType: "JSON"
        KinesisFirehoseOutput:
          ResourceARN: !GetAtt AnnotationsMonitoringS3FirehoseFailures.Arn
          RoleARN: !Ref AnalyticsOutputRoleARN

  AnnotationsMonitoringAnalyticsOutputSuccesses:
    Type: "AWS::KinesisAnalytics::ApplicationOutput"
    DependsOn: AnnotationsMonitoringAnalytics
    Properties:
      ApplicationName: !Ref AnnotationsMonitoringAnalytics
      Output:
        Name: SUCCESSFUL_PUBLISHES_STREAM
        DestinationSchema:
          RecordFormatType: "JSON"
        KinesisFirehoseOutput:
          ResourceARN: !GetAtt AnnotationsMonitoringS3FirehoseSuccesses.Arn
          RoleARN: !Ref AnalyticsOutputRoleARN

  # AnnotationsMonitoringAnalyticsOutputErrors:
    # Type: "AWS::KinesisAnalytics::ApplicationOutput"
    # DependsOn: AnnotationsMonitoringAnalytics
    # Properties:
      # ApplicationName: !Ref AnnotationsMonitoringAnalytics
      # Output:
        # Name: error_stream
        # DestinationSchema:
          # RecordFormatType: "JSON"
        # KinesisFirehoseOutput:
          # ResourceARN: !GetAtt AnnotationsMonitoringS3FirehoseErrors.Arn
          # RoleARN: !Ref AnalyticsOutputRoleARN

  AnnotationsMonitoringAnalyticsOutputPublishAnalysis:
    Type: "AWS::KinesisAnalytics::ApplicationOutput"
    DependsOn: AnnotationsMonitoringAnalytics
    Properties:
      ApplicationName: !Ref AnnotationsMonitoringAnalytics
      Output:
        Name: PUBLISH_ANALYSIS_STREAM
        DestinationSchema:
          RecordFormatType: "JSON"
        KinesisFirehoseOutput:
          ResourceARN: !GetAtt AnnotationsMonitoringS3FirehosePublishAnalysis.Arn
          RoleARN: !Ref AnalyticsOutputRoleARN
